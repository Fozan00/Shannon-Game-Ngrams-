{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muhammad Fozan\n",
    "# 19I-0507\n",
    "\n",
    "# Reading tweets from file\n",
    "# Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import string\n",
    "df = pd.read_excel('realdonaldtrump.csv.xlsx')\n",
    "\n",
    "# making list of tweets\n",
    "tweets = df['content'].copy()\n",
    "no_links = []\n",
    "\n",
    "# Removing links\n",
    "\n",
    "for row in tweets:\n",
    "    clear_txt =re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', row, flags=re.MULTILINE)\n",
    "    clear_txt = re.sub('pic.\\S+', '', clear_txt)\n",
    "    no_links.append(clear_txt)\n",
    "\n",
    "print(no_links)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "# 80% train 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, testing = train_test_split(no_links, test_size=0.2, random_state=1, shuffle=False)\n",
    "print(train)\n",
    "# train = no_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Removing All punctuations expect '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "\n",
    "content = []\n",
    "for row in train:\n",
    "  file_p = \"\".join([char for char in row if char not in string.punctuation])\n",
    "  content.append(file_p)\n",
    "\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_sents = []\n",
    "for row in content:\n",
    "    sentence = sent_tokenize(row)\n",
    "    if sentence != \"\":\n",
    "        token_sents.extend(sentence)\n",
    "token_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_with_nl_removed = \"\"\n",
    "# for line in content:\n",
    "#   line_with_nl_removed = line.replace(\"\\n\", \" \")\n",
    "#   file_with_nl_removed += line_with_nl_removed\n",
    "# file_with_nl_removed += \"\\n\"\n",
    "# print(file_with_nl_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.resource_sharer import stop\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filter_rows = []\n",
    "for row in range(len(token_sents)):\n",
    "    word = word_tokenize(token_sents[row])\n",
    "\n",
    "    st_word = \"\"\n",
    "    for j in word:\n",
    "    # if curr word is not stopword then add it to filtered\n",
    "        if j.casefold() not in stop_words:\n",
    "            st_word+=' '\n",
    "            st_word += j\n",
    "    \n",
    "    filter_rows.append(st_word)\n",
    "\n",
    "# Again removing punctuations for . and -\n",
    "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'+'.'+'–'\n",
    "\n",
    "filter_text = []\n",
    "for row in filter_rows:\n",
    "  file_p = \"\".join([char for char in row if char not in string.punctuation])\n",
    "  filter_text.append(file_p)\n",
    "\n",
    "print(filter_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Building Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "bigrams_data = []\n",
    "\n",
    "for row in filter_text:\n",
    "    data = row.split(\" \")\n",
    "    bigrams = ngrams(data, 2)\n",
    "\n",
    "    for i in bigrams:\n",
    "        bigrams_data.append(i)\n",
    "\n",
    "stops = [\"\"]\n",
    "new_bigrams = [gram for gram in bigrams_data if not any(stop in gram for stop in stops)]\n",
    "# print(new_bigrams)\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:6 Find Frequencies and make histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making frequency distribution of bigrams\n",
    "bi_freq_dist = nltk.FreqDist(new_bigrams)\n",
    "bi_freq_common = bi_freq_dist.most_common(40)\n",
    "bi_freq_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_values = []\n",
    "for tuple in bi_freq_common:\n",
    "    bi_values.append(tuple[1])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(bi_values,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:5 Making trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_data = []\n",
    "\n",
    "for row in filter_text:\n",
    "    data = row.split(\" \")\n",
    "    trigrams = ngrams(data, 3)\n",
    "\n",
    "    for i in trigrams:\n",
    "        trigrams_data.append(i)\n",
    "\n",
    "# Removing trigrams involving empty strings\n",
    "stops = [\"\"]\n",
    "new_trigrams = [gram for gram in trigrams_data if not any(stop in gram for stop in stops)]\n",
    "print(new_trigrams)\n",
    "# print(trigrams_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:6 Make Frequency Dist and Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_freq_dist = nltk.FreqDist(new_trigrams)\n",
    "tr_freq_common = tri_freq_dist.most_common(40)\n",
    "tr_freq_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making histogram of most commin 40\n",
    "tri_values = []\n",
    "for tuple in tr_freq_common:\n",
    "    tri_values.append(tuple[1])\n",
    "\n",
    "# tri_values\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(tri_values,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:7 Laplace smoothing for bigrams and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bi_freq_dist.keys():\n",
    "    bi_freq_dist[word] += 1\n",
    "\n",
    "\n",
    "for word in tri_freq_dist.keys():\n",
    "    tri_freq_dist[word] += 1\n",
    "tri_freq_dist.values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part: 8 Build dictionaries for bigram and trigram and sort in increasing order\n",
    "# For Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs = {\"Word\":[], \"Probability\":[]}\n",
    "bigram_voc = set(bi_freq_dist)\n",
    "total_freq = 0\n",
    "for word in bi_freq_dist.keys():\n",
    "    total_freq += bi_freq_dist[word]\n",
    "\n",
    "for word in bi_freq_dist.keys():\n",
    "    probability = (bi_freq_dist[word]/(total_freq + len(bigram_voc)))\n",
    "    bigram_probs[\"Word\"].append(word)\n",
    "    bigram_probs[\"Probability\"].append(probability)\n",
    "\n",
    "\n",
    "# print(type(bigram_probs))\n",
    "bigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dictionary in increasing order \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "idx = np.argsort(bigram_probs[\"Probability\"])\n",
    "d = np.dtype(int)\n",
    "bigram_probs[\"Probability\"] = list(np.array(bigram_probs[\"Probability\"])[idx])\n",
    "dt=np.dtype('object,object')\n",
    "bigram_probs[\"Word\"] = list(np.array(bigram_probs[\"Word\"],dtype=dt)[idx])\n",
    "print(bigram_probs[\"Word\"][len(bigram_probs[\"Word\"])-1])\n",
    "print(bigram_probs[\"Probability\"][len(bigram_probs[\"Word\"])-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part: 8 Build dictionaries for bigram and trigram and sort in increasing order\n",
    "# For Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_probs = {\"Word\":[], \"Probability\":[]}\n",
    "trigram_voc = set(tri_freq_dist)\n",
    "total_freq = 0\n",
    "for word in tri_freq_dist.keys():\n",
    "    total_freq += tri_freq_dist[word]\n",
    "\n",
    "for word in tri_freq_dist.keys():\n",
    "    probability = (tri_freq_dist[word]/(total_freq + len(trigram_voc)))\n",
    "    trigram_probs[\"Word\"].append(word)\n",
    "    trigram_probs[\"Probability\"].append(probability)\n",
    "\n",
    "trigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dictionary in increasing order\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "idx = np.argsort(trigram_probs[\"Probability\"])\n",
    "d = np.dtype(int)\n",
    "trigram_probs[\"Probability\"] = list(np.array(trigram_probs[\"Probability\"])[idx])\n",
    "dt=np.dtype('object,object,object')\n",
    "trigram_probs[\"Word\"] = list(np.array(trigram_probs[\"Word\"],dtype=dt)[idx])\n",
    "print(trigram_probs[\"Word\"][len(trigram_probs[\"Word\"])-1])\n",
    "print(trigram_probs[\"Probability\"][len(trigram_probs[\"Word\"])-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:10 Redo the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_freq_common = bi_freq_dist.most_common(40)\n",
    "bi_values = []\n",
    "for tuple in bi_freq_common:\n",
    "    bi_values.append(tuple[1])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(bi_values,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:10 Redo the Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_freq_common = tri_freq_dist.most_common(40)\n",
    "tri_values = []\n",
    "for tuple in tr_freq_common:\n",
    "    tri_values.append(tuple[1])\n",
    "\n",
    "# tri_values\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(tri_values,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building unigrams to be used in Part 11\n",
    "unigrams=[]\n",
    "for sent in filter_text:\n",
    "    word_sequence=word_tokenize(sent)\n",
    "    for word in word_sequence:\n",
    "        if (word == '.'):\n",
    "            word_sequence.remove(word)\n",
    "        else:\n",
    "            unigrams.append(word)\n",
    "\n",
    "frequency_unigrams=nltk.FreqDist(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:11 Take a four-word string as the test sample and find both its unigram, bigram, and trigram perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str4=\"Did you know Donald\"\n",
    "testset=word_tokenize(str4)\n",
    "\n",
    "#unigram\n",
    "perplexity = 1\n",
    "N = 0\n",
    "for word in testset:\n",
    "    if word in frequency_unigrams.keys():\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/frequency_unigrams[word])\n",
    "if N == 0:\n",
    "    perplexity = 1\n",
    "else :\n",
    "    perplexity = pow(perplexity, 1/float(N))\n",
    "\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= str4.split(\" \")\n",
    "test_bigrm=ngrams(data,2)\n",
    "\n",
    "perplexity = 1\n",
    "N = 0\n",
    "\n",
    "list=[]\n",
    "list.extend(test_bigrm)\n",
    "\n",
    "for word in list:\n",
    "    if word in bi_freq_dist.keys():\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/bi_freq_dist[word])\n",
    "if N == 0:\n",
    "    perplexity = 1\n",
    "else :\n",
    "    perplexity = pow(perplexity, 1/float(N))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= str4.split(\" \")\n",
    "test_trigrm=ngrams(data,3)\n",
    "\n",
    "perplexity = 1\n",
    "N = 0\n",
    "\n",
    "list=[]\n",
    "list.extend(test_trigrm)\n",
    "\n",
    "\n",
    "for word in list:\n",
    "    if word in tri_freq_dist.keys():\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/tri_freq_dist[word])\n",
    "if N == 0:\n",
    "    perplexity = 1\n",
    "else :\n",
    "    perplexity = pow(perplexity, 1/float(N))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:12 Natural Langauge Generation Task: Based on step 7, build CDF (Cumulative Distribution Function). Generate a random number and compare it with the CDF. Pick the word whose CDF value is closest to it and return it as the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N=len(trigram_probs[\"Probability\"])\n",
    "data=trigram_probs[\"Probability\"]\n",
    "plt.xlabel('probability')\n",
    "plt.ylabel('y-axis')\n",
    "x=np.sort(data)\n",
    "y=np.arange(N)/float(N)\n",
    "plt.plot(x, y, marker='o',label='trigram')\n",
    "num = random.uniform(0, max(x))\n",
    "nearest=x[min(range(len(x)), key = lambda i: abs(x[i]-num))]\n",
    "ind1=np.where(x==nearest)\n",
    "\n",
    "N=len(bigram_probs[\"Probability\"])\n",
    "data=bigram_probs[\"Probability\"]\n",
    "x=np.sort(data)\n",
    "y=np.arange(N)/float(N)\n",
    "\n",
    "plt.plot(x, y, marker='x',label=\"'bigram'\")\n",
    "plt.title('CDF (Cumulative Distribution Function)')\n",
    "num = random.uniform(0, max(x))\n",
    "print(\"Random Number : \", num)\n",
    "nearest=x[min(range(len(x)), key = lambda i: abs(x[i]-num))]\n",
    "ind=np.where(x==nearest)\n",
    "print(\"Closest Bigram : \",bigram_probs[\"Word\"][ind[0][0]])\n",
    "print(\"Closest Trigram : \",trigram_probs[\"Word\"][ind1[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part:13 Shannon Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—' + '.' + '…'\n",
    "\n",
    "test_content = []\n",
    "for row in testing:\n",
    "  file_p = \"\".join([char for char in row if char not in string.punctuation])\n",
    "  test_content.append(file_p)\n",
    "\n",
    "test_tokens = []\n",
    "for row in test_content:\n",
    "    sentence = sent_tokenize(row)\n",
    "    if sentence != \"\":\n",
    "        test_tokens.extend(sentence)\n",
    "# Preparing Test Y for comparison\n",
    "test_Y = []\n",
    "for sent in test_tokens:\n",
    "  last_word = \"\"\n",
    "  for ch in range(len(sent)):\n",
    "    \n",
    "    if sent[ch] == ' ':\n",
    "      last_word = \"\"\n",
    "    else:\n",
    "      last_word += sent[ch]\n",
    "  test_Y.append(last_word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd last words\n",
    "sec_lasts = []\n",
    "for sent in test_tokens:\n",
    "  last2 = \"\"\n",
    "  counter = len(sent)-1\n",
    "  start = 0\n",
    "  while counter >= 0:\n",
    "    if start and sent[counter]==\" \":\n",
    "      break\n",
    "    elif start:\n",
    "      last2 += sent[counter]\n",
    "    if(sent[counter]==\" \"):\n",
    "      start = 1\n",
    "    \n",
    "\n",
    "    counter -= 1\n",
    "  sec_lasts.append(last2[::-1])\n",
    "print(sec_lasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_output = []\n",
    "all_keys = bi_freq_dist.keys()\n",
    "all_values = bi_freq_dist.values()\n",
    "\n",
    "print(all_keys)\n",
    "# for word in sec_lasts:\n",
    "#     # find all bigrams\n",
    "#     freq = 0\n",
    "#     if word == \"\":\n",
    "#         bi_output.append(\"UNKNOWN\")\n",
    "#     else:\n",
    "#         for i in range(len(all_keys)):\n",
    "#             j = i\n",
    "#             # if all_keys[i][0] == word:\n",
    "#     #             if all_values[i] > freq:\n",
    "#     #                 bi_predict = all_keys[i][0]\n",
    "#     #                 freq = all_values[i]\n",
    "#         # if freq == 0:\n",
    "#         #     bi_output.append(\"UNKNOWN\")\n",
    "#         # else:\n",
    "#         #     bi_output.append(bi_predict)\n",
    "# print(bi_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
